{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/cisti2025_earlysymbolicknowledge/blob/main/general_simbolico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c74d93-0d18-4ab0-954a-8b648bd3128d",
      "metadata": {
        "id": "15c74d93-0d18-4ab0-954a-8b648bd3128d"
      },
      "source": [
        "# Early Injection of Symbolic Concepts (Experiment 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744f09a7-d21b-4d52-b7c7-edc5aa13c11f",
      "metadata": {
        "id": "744f09a7-d21b-4d52-b7c7-edc5aa13c11f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import Word2Vec\n",
        "import json\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520192e4-dfb5-4767-9230-94509a52bfa5",
      "metadata": {
        "id": "520192e4-dfb5-4767-9230-94509a52bfa5"
      },
      "outputs": [],
      "source": [
        "# Cargar frases desde los archivos JSON\n",
        "path = \"D://2025 hot writing/pp_symbolic_knowledge_LLM/\"\n",
        "\n",
        "with open(path+\"general_phrases.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    general_data = json.load(f)\n",
        "    general_sentences = [sentence.split() for sentence in general_data[\"phrases\"]]\n",
        "\n",
        "with open(path+\"spatial_phrases.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    spatial_data = json.load(f)\n",
        "    spatial_sentences = [sentence.split() for sentence in spatial_data[\"spatial_data\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b6c5d1-3792-4ea4-9c36-fb191ee34df1",
      "metadata": {
        "id": "38b6c5d1-3792-4ea4-9c36-fb191ee34df1",
        "outputId": "715530ad-72ea-4cc8-bd45-62a5fce5ac26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PC}\\AppData\\Local\\Temp\\ipykernel_11912\\431791414.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
            "  spatial_embeddings = torch.tensor([word2vec_symbolic.wv[word] for word in spatial_words if word in word2vec_symbolic.wv])\n"
          ]
        }
      ],
      "source": [
        "# Entrenar modelos Word2Vec para el modelo base y el simbólico\n",
        "word2vec_general = Word2Vec(sentences=general_sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "combined_sentences = general_sentences + spatial_sentences\n",
        "word2vec_symbolic = Word2Vec(sentences=combined_sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Obtener embeddings de palabras clave espaciales\n",
        "spatial_words = [\"movimiento\", \"rápido\", \"borde\", \"encima\", \"debajo\", \"cerca\", \"lejos\"]\n",
        "spatial_embeddings = torch.tensor([word2vec_symbolic.wv[word] for word in spatial_words if word in word2vec_symbolic.wv])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518fbaa8-fad1-4cfb-92eb-277b6969eebe",
      "metadata": {
        "id": "518fbaa8-fad1-4cfb-92eb-277b6969eebe"
      },
      "outputs": [],
      "source": [
        "# Definir embeddings simbólicos\n",
        "class SpatialEmbedding(nn.Module):\n",
        "    def __init__(self, embed_size, spatial_embeddings):\n",
        "        super(SpatialEmbedding, self).__init__()\n",
        "        self.embedding = nn.Parameter(spatial_embeddings.mean(dim=0))  # Centroide de los embeddings\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.embedding\n",
        "\n",
        "# Modelo simple basado en embeddings simbólicos\n",
        "class SymbolicModel(nn.Module):\n",
        "    def __init__(self, embed_size=50):\n",
        "        super(SymbolicModel, self).__init__()\n",
        "        self.spatial_embedding = SpatialEmbedding(embed_size, spatial_embeddings)\n",
        "        self.fc = nn.Linear(embed_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.spatial_embedding(x)\n",
        "        x = self.fc(x.mean(dim=1))  # Promedio de embeddings\n",
        "        return torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b0fab3-5c37-450c-9ab6-e5f324f295c1",
      "metadata": {
        "id": "63b0fab3-5c37-450c-9ab6-e5f324f295c1"
      },
      "outputs": [],
      "source": [
        "# Función para codificar y hacer padding de las oraciones\n",
        "def encode_sentences(sentences, model, max_len):\n",
        "    encoded = []\n",
        "    for sentence in sentences:\n",
        "        vecs = [model.wv[word] for word in sentence if word in model.wv]\n",
        "        if len(vecs) < max_len:\n",
        "            vecs += [np.zeros(model.vector_size)] * (max_len - len(vecs))  # Padding con ceros\n",
        "        encoded.append(vecs[:max_len])  # Truncar si excede max_len\n",
        "    return torch.tensor(encoded, dtype=torch.float32)\n",
        "\n",
        "def find_closest_sentences(test_embedding, sentences, model, top_n=3):\n",
        "    sentence_embeddings = [np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0) for sentence in sentences]\n",
        "    similarities = [(i, 1 - cosine(test_embedding, emb)) for i, emb in enumerate(sentence_embeddings) if emb is not None]\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [\" \".join(sentences[i]) for i, _ in similarities[:top_n]]\n",
        "\n",
        "max_length = max(len(sentence) for sentence in general_sentences + spatial_sentences)\n",
        "\n",
        "general_data = encode_sentences(general_sentences, word2vec_general, max_length)\n",
        "spatial_data = encode_sentences(spatial_sentences, word2vec_symbolic, max_length)\n",
        "\n",
        "# Asegurar que las etiquetas tengan la misma longitud que los datos\n",
        "labels_general = torch.zeros(len(general_data), dtype=torch.float32)\n",
        "labels_spatial = torch.ones(len(spatial_data), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e752baaa-8ac3-4ce7-bbb0-0d30a7e4b0bc",
      "metadata": {
        "id": "e752baaa-8ac3-4ce7-bbb0-0d30a7e4b0bc",
        "outputId": "8b10dca4-8c29-471b-9d45-8cfff10b97b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Modelo Base] Epoch 1, Loss: 0.6714127659797668\n",
            "[Modelo Base] Epoch 2, Loss: 0.6620928645133972\n",
            "[Modelo Base] Epoch 3, Loss: 0.6526336073875427\n",
            "[Modelo Base] Epoch 4, Loss: 0.6427007913589478\n",
            "[Modelo Base] Epoch 5, Loss: 0.6325231194496155\n",
            "[Modelo Base] Epoch 6, Loss: 0.6224822402000427\n",
            "[Modelo Base] Epoch 7, Loss: 0.6130567193031311\n",
            "[Modelo Base] Epoch 8, Loss: 0.6048005223274231\n",
            "[Modelo Base] Epoch 9, Loss: 0.5983045697212219\n",
            "[Modelo Base] Epoch 10, Loss: 0.594116747379303\n",
            "[Modelo Base] Epoch 11, Loss: 0.592588484287262\n",
            "[Modelo Base] Epoch 12, Loss: 0.5936216115951538\n",
            "[Modelo Base] Epoch 13, Loss: 0.5964096188545227\n",
            "[Modelo Base] Epoch 14, Loss: 0.5995622277259827\n",
            "[Modelo Base] Epoch 15, Loss: 0.6018022894859314\n",
            "[Modelo Base] Epoch 16, Loss: 0.6025369167327881\n",
            "[Modelo Base] Epoch 17, Loss: 0.6018377542495728\n",
            "[Modelo Base] Epoch 18, Loss: 0.6001458764076233\n",
            "[Modelo Base] Epoch 19, Loss: 0.598008394241333\n",
            "[Modelo Base] Epoch 20, Loss: 0.5959154367446899\n",
            "[Modelo Base] Epoch 21, Loss: 0.5942177176475525\n",
            "[Modelo Base] Epoch 22, Loss: 0.5930982828140259\n",
            "[Modelo Base] Epoch 23, Loss: 0.5925842523574829\n",
            "[Modelo Base] Epoch 24, Loss: 0.5925842523574829\n",
            "[Modelo Base] Epoch 25, Loss: 0.5929376482963562\n",
            "[Modelo Base] Epoch 26, Loss: 0.5934641361236572\n",
            "[Modelo Base] Epoch 27, Loss: 0.5940011143684387\n",
            "[Modelo Base] Epoch 28, Loss: 0.5944265127182007\n",
            "[Modelo Base] Epoch 29, Loss: 0.5946667194366455\n",
            "[Modelo Base] Epoch 30, Loss: 0.5946950912475586\n",
            "[Modelo Simbólico] Epoch 1, Loss: 0.7068380117416382\n",
            "[Modelo Simbólico] Epoch 2, Loss: 0.6975014805793762\n",
            "[Modelo Simbólico] Epoch 3, Loss: 0.6875406503677368\n",
            "[Modelo Simbólico] Epoch 4, Loss: 0.6764857172966003\n",
            "[Modelo Simbólico] Epoch 5, Loss: 0.6644991040229797\n",
            "[Modelo Simbólico] Epoch 6, Loss: 0.651889979839325\n",
            "[Modelo Simbólico] Epoch 7, Loss: 0.6390892863273621\n",
            "[Modelo Simbólico] Epoch 8, Loss: 0.6266362071037292\n",
            "[Modelo Simbólico] Epoch 9, Loss: 0.6151626110076904\n",
            "[Modelo Simbólico] Epoch 10, Loss: 0.6053653359413147\n",
            "[Modelo Simbólico] Epoch 11, Loss: 0.5979484915733337\n",
            "[Modelo Simbólico] Epoch 12, Loss: 0.5935087203979492\n",
            "[Modelo Simbólico] Epoch 13, Loss: 0.5923358201980591\n",
            "[Modelo Simbólico] Epoch 14, Loss: 0.5941370129585266\n",
            "[Modelo Simbólico] Epoch 15, Loss: 0.5978448987007141\n",
            "[Modelo Simbólico] Epoch 16, Loss: 0.6018590927124023\n",
            "[Modelo Simbólico] Epoch 17, Loss: 0.6047561168670654\n",
            "[Modelo Simbólico] Epoch 18, Loss: 0.6058365106582642\n",
            "[Modelo Simbólico] Epoch 19, Loss: 0.6051287651062012\n",
            "[Modelo Simbólico] Epoch 20, Loss: 0.6031049489974976\n",
            "[Modelo Simbólico] Epoch 21, Loss: 0.6003989577293396\n",
            "[Modelo Simbólico] Epoch 22, Loss: 0.5976155996322632\n",
            "[Modelo Simbólico] Epoch 23, Loss: 0.5952224135398865\n",
            "[Modelo Simbólico] Epoch 24, Loss: 0.5935018658638\n",
            "[Modelo Simbólico] Epoch 25, Loss: 0.592548131942749\n",
            "[Modelo Simbólico] Epoch 26, Loss: 0.5922984480857849\n",
            "[Modelo Simbólico] Epoch 27, Loss: 0.5925849676132202\n",
            "[Modelo Simbólico] Epoch 28, Loss: 0.5931923985481262\n",
            "[Modelo Simbólico] Epoch 29, Loss: 0.5939085483551025\n",
            "[Modelo Simbólico] Epoch 30, Loss: 0.5945582985877991\n",
            "['El', 'plato', 'podría', 'caerse', 'de', 'la', 'mesa', 'por', 'un', 'descuido']\n",
            "Predicción del Modelo Base: 0.307899534702301\n",
            "Predicción del Modelo Simbólico: 0.3135566711425781\n",
            "Frases más cercanas en general: ['El plato más cercano al borde de la mesa es el de pan.', 'El plato de postre está más alejado del centro de la mesa.', 'El plato de cerámica tiene un borde decorado.']\n",
            "Frases más cercanas en simbólico: ['Si la mesa está desnivelada, un plato puede deslizarse hacia un lado.', 'Un plato que golpea el borde de la mesa puede rebotar y caer al suelo.', 'Si un plato queda en el filo de la mesa, cualquier movimiento lo puede hacer caer.']\n"
          ]
        }
      ],
      "source": [
        "# Entrenar modelo base\n",
        "EPOCHS = 30\n",
        "model_base = SymbolicModel()\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "data_combined = torch.cat([general_data, spatial_data], dim=0)\n",
        "labels_combined = torch.cat([labels_general, labels_spatial], dim=0)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_base(data_combined).squeeze()\n",
        "    loss = loss_fn(outputs, labels_combined)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[Modelo Base] Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Entrenar modelo simbólico\n",
        "model_symbolic = SymbolicModel()\n",
        "optimizer = optim.Adam(model_symbolic.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_symbolic(data_combined).squeeze()\n",
        "    loss = loss_fn(outputs, labels_combined)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[Modelo Simbólico] Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluación con una frase nueva\n",
        "test_sentence_orig = \"El plato podría caerse de la mesa por un descuido\"\n",
        "test_sentence = test_sentence_orig.split(' ')\n",
        "print(test_sentence)\n",
        "test_data_general = encode_sentences([test_sentence], word2vec_general, max_length)\n",
        "test_data_symbolic = encode_sentences([test_sentence], word2vec_symbolic, max_length)\n",
        "test_embed_general = np.mean([word2vec_general.wv[word] for word in test_sentence if word in word2vec_general.wv], axis=0)\n",
        "test_embed_symbolic = np.mean([word2vec_symbolic.wv[word] for word in test_sentence if word in word2vec_symbolic.wv], axis=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction_base = model_base(test_data_general).item()\n",
        "    prediction_symbolic = model_symbolic(test_data_symbolic).item()\n",
        "    closest_general = find_closest_sentences(test_embed_general, general_sentences, word2vec_general)\n",
        "    closest_symbolic = find_closest_sentences(test_embed_symbolic, spatial_sentences, word2vec_symbolic)\n",
        "\n",
        "\n",
        "print(\"Predicción del Modelo Base:\", prediction_base)\n",
        "print(\"Predicción del Modelo Simbólico:\", prediction_symbolic)\n",
        "print(\"Frases más cercanas en general:\", closest_general)\n",
        "print(\"Frases más cercanas en simbólico:\", closest_symbolic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c64003-80dd-49bf-b7bb-6f0130888609",
      "metadata": {
        "id": "a6c64003-80dd-49bf-b7bb-6f0130888609"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}